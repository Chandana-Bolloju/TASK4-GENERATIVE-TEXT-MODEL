import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class TextGenerator:
    def __init__(self):
        # Initialize GPT-2 components
        self.gpt2_tokenizer = None
        self.gpt2_model = None
        
        # Initialize LSTM components
        self.lstm_model = None
        self.lstm_tokenizer = None
        self.max_sequence_len = 0
        self.total_words = 0
        
    def initialize_gpt(self):
        """Initialize GPT-2 model and tokenizer"""
        print("Loading GPT-2 model...")
        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2-medium')
        print("GPT-2 model loaded successfully!")
    
    def generate_with_gpt(self, prompt, max_length=150, temperature=0.7):
        """
        Generate text using GPT-2 model
        Args:
            prompt (str): Starting text for generation
            max_length (int): Maximum length of generated text
            temperature (float): Controls randomness (lower = more deterministic)
        Returns:
            str: Generated text
        """
        if self.gpt2_model is None:
            self.initialize_gpt()
            
        inputs = self.gpt2_tokenizer.encode(prompt, return_tensors='pt')
        
        outputs = self.gpt2_model.generate(
            inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            no_repeat_ngram_size=2,
            pad_token_id=self.gpt2_tokenizer.eos_token_id
        )
        
        return self.gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    def train_lstm(self, texts, epochs=100):
        """
        Train LSTM model on provided texts
        Args:
            texts (list): List of training texts
            epochs (int): Number of training epochs
        """
        print("Training LSTM model...")
        
        # Tokenize text
        self.lstm_tokenizer = Tokenizer()
        self.lstm_tokenizer.fit_on_texts(texts)
        self.total_words = len(self.lstm_tokenizer.word_index) + 1
        
        # Create input sequences
        input_sequences = []
        for line in texts:
            token_list = self.lstm_tokenizer.texts_to_sequences([line])[0]
            for i in range(1, len(token_list)):
                n_gram_sequence = token_list[:i+1]
                input_sequences.append(n_gram_sequence)
        
        # Pad sequences
        self.max_sequence_len = max([len(x) for x in input_sequences])
        input_sequences = pad_sequences(input_sequences, maxlen=self.max_sequence_len, padding='pre')
        
        # Create predictors and label
        X = input_sequences[:, :-1]
        y = input_sequences[:, -1]
        y = tf.keras.utils.to_categorical(y, num_classes=self.total_words)
        
        # Build LSTM model
        self.lstm_model = Sequential([
            Embedding(self.total_words, 64, input_length=self.max_sequence_len-1),
            LSTM(100),
            Dense(self.total_words, activation='softmax')
        ])
        
        self.lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        self.lstm_model.fit(X, y, epochs=epochs, verbose=1)
        print("LSTM model trained successfully!")
    
    def generate_with_lstm(self, seed_text, next_words=20):
        """
        Generate text using LSTM model
        Args:
            seed_text (str): Starting text for generation
            next_words (int): Number of words to generate
        Returns:
            str: Generated text
        """
        if self.lstm_model is None:
            raise ValueError("LSTM model not trained. Call train_lstm() first.")
            
        output_text = seed_text
        
        for _ in range(next_words):
            token_list = self.lstm_tokenizer.texts_to_sequences([seed_text])[0]
            token_list = pad_sequences([token_list], maxlen=self.max_sequence_len-1, padding='pre')
            predicted_probs = self.lstm_model.predict(token_list, verbose=0)
            predicted_index = np.argmax(predicted_probs)
            
            output_word = ""
            for word, index in self.lstm_tokenizer.word_index.items():
                if index == predicted_index:
                    output_word = word
                    break
                    
            seed_text += " " + output_word
            output_text += " " + output_word
            
        return output_text

# Example usage
if __name__ == "__main__":
    # Initialize text generator
    generator = TextGenerator()
    
    # Example 1: Using GPT-2
    print("\n=== GPT-2 Generation ===")
    gpt_prompt = "The future of renewable energy"
    print(f"Prompt: {gpt_prompt}")
    print("Generated Text:")
    print(generator.generate_with_gpt(gpt_prompt))
    
    # Example 2: Using LSTM
    print("\n=== LSTM Generation ===")
    
    # Training data for LSTM
    training_texts = [
        "Artificial intelligence is transforming many industries",
        "Machine learning algorithms can analyze vast amounts of data",
        "Deep learning requires powerful GPUs for training complex models",
        "AI applications range from healthcare to autonomous vehicles",
        "Natural language processing enables computers to understand human language"
    ]
    
    # Train LSTM model
    generator.train_lstm(training_texts, epochs=50)
    
    # Generate text with LSTM
    lstm_seed = "Artificial intelligence"
    print(f"\nSeed text: {lstm_seed}")
    print("Generated Text:")
    print(generator.generate_with_lstm(lstm_seed, next_words=15))
